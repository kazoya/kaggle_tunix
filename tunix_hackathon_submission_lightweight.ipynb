{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Tunix Hackathon - Lightweight Version (PyTorch)\n",
    "\n",
    "This is a simplified version that:\n",
    "- ‚úÖ Works without HF_TOKEN (uses public models)\n",
    "- ‚úÖ Uses PyTorch instead of JAX (easier setup)\n",
    "- ‚úÖ Simple GRPO implementation\n",
    "- ‚úÖ Direct dataset loading from HuggingFace\n",
    "\n",
    "**Perfect for quick experiments on Kaggle!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_needed(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "# Install required packages\n",
    "packages = [\"torch\", \"transformers\", \"datasets\", \"accelerate\"]\n",
    "for pkg in packages:\n",
    "    install_if_needed(pkg)\n",
    "\n",
    "print(\"\\n‚úÖ All packages ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress transformers warnings about generation flags\n",
    "import warnings\n",
    "import os\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'  # Suppress info/warning messages\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model (Public - No Token Required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Qwen 0.5B - truly public model, no token needed\n",
    "# Alternative: google/gemma-2-2b-it (might need token)\n",
    "MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL}...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading {MODEL}: {e}\")\n",
    "    print(\"\\nTrying alternative model: microsoft/Phi-3-mini-4k-instruct\")\n",
    "    MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"‚úÖ Alternative model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load GSM8K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading GSM8K via HuggingFace...\")\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "train_data = ds[\"train\"]\n",
    "test_data = ds[\"test\"]\n",
    "print(f\"‚úÖ Train samples: {len(train_data)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(example):\n",
    "    q = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    prompt = f\"Question: {q}\\nAnswer:\"\n",
    "    \n",
    "    example[\"prompt\"] = prompt\n",
    "    example[\"target\"] = answer\n",
    "    return example\n",
    "\n",
    "train_data = train_data.map(encode)\n",
    "print(\"‚úÖ Data prepared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GRPO Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt, max_new_tokens=64):\n",
    "    \"\"\"Custom generation loop with safe sampling - NO model.generate()\"\"\"\n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        generated_ids = input_ids.clone()\n",
    "        \n",
    "        model.eval()  # Set to eval mode for generation\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(max_new_tokens):\n",
    "                # Forward pass - get logits for next token\n",
    "                outputs = model(input_ids=generated_ids)\n",
    "                next_token_logits = outputs.logits[:, -1, :]  # Shape: (batch, vocab_size)\n",
    "                \n",
    "                # ============================================================\n",
    "                # üîí Safe Sampling - Handle NaN/Inf (Your Solution)\n",
    "                # ============================================================\n",
    "                logits = next_token_logits\n",
    "                \n",
    "                # 1) Remove NaN/Inf values\n",
    "                logits = torch.nan_to_num(logits, neginf=-1e9, posinf=1e9)\n",
    "                \n",
    "                # 2) Clamp to prevent overflow\n",
    "                logits = torch.clamp(logits, -50, 50)\n",
    "                \n",
    "                # 3) Apply temperature\n",
    "                temperature = 0.7\n",
    "                logits = logits / temperature\n",
    "                \n",
    "                # 4) Stable softmax (numerically stable)\n",
    "                max_logits = logits.max(dim=-1, keepdim=True).values\n",
    "                stable_logits = logits - max_logits\n",
    "                exp_logits = torch.exp(stable_logits)\n",
    "                probs = exp_logits / exp_logits.sum(dim=-1, keepdim=True)\n",
    "                \n",
    "                # 5) Fix any remaining NaN\n",
    "                probs = torch.nan_to_num(probs, nan=0.0)\n",
    "                probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "                \n",
    "                # 6) Apply top_p filtering (nucleus sampling)\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                top_p = 0.9\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0  # Keep at least one token\n",
    "                \n",
    "                # Scatter back to original indices\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                probs[indices_to_remove] = 0.0\n",
    "                \n",
    "                # Renormalize after filtering\n",
    "                probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "                \n",
    "                # 7) Safe sampling with fallback\n",
    "                try:\n",
    "                    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "                except RuntimeError as e:\n",
    "                    # Fallback to greedy if sampling fails\n",
    "                    if step == 0:  # Only print once\n",
    "                        print(f\"‚ö†Ô∏è Sampling failed at step {step}, using greedy: {e}\")\n",
    "                    next_tokens = torch.argmax(probs, dim=-1)\n",
    "                \n",
    "                # Append to generated sequence\n",
    "                generated_ids = torch.cat([generated_ids, next_tokens.unsqueeze(1)], dim=1)\n",
    "                \n",
    "                # Stop if EOS token\n",
    "                if tokenizer.eos_token_id is not None:\n",
    "                    if next_tokens.item() == tokenizer.eos_token_id:\n",
    "                        break\n",
    "        \n",
    "        # Decode and return\n",
    "        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return text.replace(prompt, \"\").strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"[Generation Error]\"\n",
    "    finally:\n",
    "        model.train()  # Restore training mode if needed\n",
    "\n",
    "def reward_fn(pred, gold):\n",
    "    \"\"\"Simple reward: 1 if correct answer number is in prediction\"\"\"\n",
    "    if pred == \"[Generation Error]\" or not pred:\n",
    "        return 0.0\n",
    "    try:\n",
    "        gold_num = gold.split(\"####\")[-1].strip()\n",
    "        return 1.0 if gold_num in pred else 0.0\n",
    "    except:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GRPO Training (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick model health check\n",
    "print(\"Testing model generation...\")\n",
    "test_prompt = \"Question: What is 2+2?\\nAnswer:\"\n",
    "try:\n",
    "    test_output = generate_answer(test_prompt, max_new_tokens=20)\n",
    "    print(f\"‚úÖ Model test successful: {test_output[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model test failed: {e}\")\n",
    "    print(\"Please check model loading or try a different model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "EPOCHS = 1\n",
    "GROUP = 2   # Number of samples per step\n",
    "STEPS = 200  # Number of training steps\n",
    "\n",
    "print(f\"Starting GRPO training...\")\n",
    "print(f\"Epochs: {EPOCHS}, Steps: {STEPS}, Group size: {GROUP}\")\n",
    "\n",
    "model.train()  # Set to training mode\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n=== Epoch {epoch+1} ===\")\n",
    "    \n",
    "    for idx in range(STEPS):\n",
    "        # Sample random batch\n",
    "        batch = [train_data[random.randint(0, len(train_data)-1)] for _ in range(GROUP)]\n",
    "        \n",
    "        prompts = [b[\"prompt\"] for b in batch]\n",
    "        golds   = [b[\"target\"] for b in batch]\n",
    "        \n",
    "        # Generate predictions\n",
    "        preds = [generate_answer(p) for p in prompts]\n",
    "        rewards = [reward_fn(preds[i], golds[i]) for i in range(GROUP)]\n",
    "        \n",
    "        # Calculate advantage relative to group average\n",
    "        avg_reward = sum(rewards) / GROUP\n",
    "        advantages = [r - avg_reward for r in rewards]\n",
    "        \n",
    "        # Calculate losses - FIXED: use proper loss calculation\n",
    "        losses = []\n",
    "        for i in range(GROUP):\n",
    "            # Tokenize prompt\n",
    "            inp = tokenizer(prompts[i], return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            # Tokenize generated prediction as target\n",
    "            pred_tokens = tokenizer(preds[i], return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "            \n",
    "            # Concatenate prompt and prediction for full sequence\n",
    "            full_input_ids = torch.cat([inp[\"input_ids\"], pred_tokens[\"input_ids\"]], dim=1)\n",
    "            \n",
    "            # Create labels (only compute loss on generated part)\n",
    "            labels = full_input_ids.clone()\n",
    "            labels[:, :inp[\"input_ids\"].shape[1]] = -100  # Ignore prompt in loss\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=full_input_ids, labels=labels)\n",
    "            logits = outputs.loss\n",
    "            \n",
    "            # Weight by advantage (GRPO)\n",
    "            losses.append(logits * advantages[i])\n",
    "        \n",
    "        # Backward pass\n",
    "        loss = sum(losses) / GROUP\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"‚ö†Ô∏è Step {idx}: Invalid loss, skipping...\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if idx % 20 == 0:\n",
    "            print(f\"Step {idx}, Loss = {loss.item():.4f}, Rewards = {rewards}, Avg Reward = {avg_reward:.2f}\")\n",
    "\n",
    "print(\"\\nüéâ Training Done!\")\n",
    "model.eval()  # Set back to eval mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test on Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample\n",
    "sample = test_data[0]\n",
    "prompt = f\"Question: {sample['question']}\\nAnswer:\"\n",
    "prediction = generate_answer(prompt)\n",
    "\n",
    "print(\"Question:\", sample['question'])\n",
    "print(\"\\nPrediction:\", prediction)\n",
    "print(\"\\nGround Truth:\", sample['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}