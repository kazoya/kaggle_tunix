{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Tunix Hackathon - Lightweight Version (PyTorch)\n",
    "\n",
    "This is a simplified version that:\n",
    "- âœ… Works without HF_TOKEN (uses public models)\n",
    "- âœ… Uses PyTorch instead of JAX (easier setup)\n",
    "- âœ… Simple GRPO implementation\n",
    "- âœ… Direct dataset loading from HuggingFace\n",
    "\n",
    "**Perfect for quick experiments on Kaggle!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model (Public - No Token Required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Gemma 2B - public model, no token needed\n",
    "MODEL = \"google/gemma-2-2b-it\"\n",
    "\n",
    "print(f\"Loading {MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"âœ… Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load GSM8K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading GSM8K via HuggingFace...\")\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "train_data = ds[\"train\"]\n",
    "test_data = ds[\"test\"]\n",
    "print(f\"âœ… Train samples: {len(train_data)}\")\n",
    "print(f\"âœ… Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(example):\n",
    "    q = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    prompt = f\"Question: {q}\\nAnswer:\"\n",
    "    \n",
    "    example[\"prompt\"] = prompt\n",
    "    example[\"target\"] = answer\n",
    "    return example\n",
    "\n",
    "train_data = train_data.map(encode)\n",
    "print(\"âœ… Data prepared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GRPO Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt, max_new_tokens=64):\n",
    "    \"\"\"Generate answer from model\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return text.replace(prompt, \"\").strip()\n",
    "\n",
    "def reward_fn(pred, gold):\n",
    "    \"\"\"Simple reward: 1 if correct answer number is in prediction\"\"\"\n",
    "    gold_num = gold.split(\"####\")[-1].strip()\n",
    "    return 1.0 if gold_num in pred else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GRPO Training (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "EPOCHS = 1\n",
    "GROUP = 2   # Number of samples per step\n",
    "STEPS = 200  # Number of training steps\n",
    "\n",
    "print(f\"Starting GRPO training...\")\n",
    "print(f\"Epochs: {EPOCHS}, Steps: {STEPS}, Group size: {GROUP}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n=== Epoch {epoch+1} ===\")\n",
    "    \n",
    "    for idx in range(STEPS):\n",
    "        # Sample random batch\n",
    "        batch = [train_data[random.randint(0, len(train_data)-1)] for _ in range(GROUP)]\n",
    "        \n",
    "        prompts = [b[\"prompt\"] for b in batch]\n",
    "        golds   = [b[\"target\"] for b in batch]\n",
    "        \n",
    "        # Generate predictions\n",
    "        preds = [generate_answer(p) for p in prompts]\n",
    "        rewards = [reward_fn(preds[i], golds[i]) for i in range(GROUP)]\n",
    "        \n",
    "        # Calculate advantage relative to group average\n",
    "        avg_reward = sum(rewards) / GROUP\n",
    "        advantages = [r - avg_reward for r in rewards]\n",
    "        \n",
    "        # Calculate losses\n",
    "        losses = []\n",
    "        for i in range(GROUP):\n",
    "            inp = tokenizer(prompts[i], return_tensors=\"pt\").to(model.device)\n",
    "            out = tokenizer(preds[i], return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inp, labels=out[\"input_ids\"])\n",
    "            logits = outputs.loss\n",
    "            \n",
    "            # Weight by advantage\n",
    "            losses.append(logits * advantages[i])\n",
    "        \n",
    "        # Backward pass\n",
    "        loss = sum(losses) / GROUP\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if idx % 20 == 0:\n",
    "            print(f\"Step {idx}, Loss = {loss.item():.4f}, Rewards = {rewards}, Avg Reward = {avg_reward:.2f}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test on Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample\n",
    "sample = test_data[0]\n",
    "prompt = f\"Question: {sample['question']}\\nAnswer:\"\n",
    "prediction = generate_answer(prompt)\n",
    "\n",
    "print(\"Question:\", sample['question'])\n",
    "print(\"\\nPrediction:\", prediction)\n",
    "print(\"\\nGround Truth:\", sample['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}